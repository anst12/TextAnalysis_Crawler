---
title: "Text_analysis 4.1 Preprocessing"
author: "Sung Tae An"
date: '2020. 2. 23.'
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, eval = TRUE, warning = FALSE)
```


## 텍스트 데이터 전처리

- 학습목표: tm 패키지를 사용하여 말뭉치 텍스트 데이터를 연구에 적합한 형태로 전처리한다.

### 텍스트 마이닝 기본 개념
- 텍스트 마이닝의 대상이 되는 전체 텍스트 데이터는 말뭉치(corpus)라고 불린다. 말뭉치는 "대용량의 정형화된 텍스트 집합(large and structured set of texts)"으로 정의된다(Minder, Elder and Hill, 2012; 1018).
1. 대용량? 모집단에 따라 다르다.
2. 정형화? 음운(phoneme), 단어(word), 문법적 변용(be동사), 어순?

- 텍스트 전처리란? 정형화 되어있지 않은 텍스트를 정형화 시키는 과정이다. 
cf. 수치화된 자료에 대한 재코딩 또는 변환 과정과 동일함.


#### 1. 공란 처리
수집 과정의 실수 또는 자료의 문제로 인해 공란이 2개 이상 발생한 경우 공란을 하나로 축소하는 과정이다.

만약 공란을 처리하지 않으면 어떤 문제가 발생할 수 있는가?
```{r}
texts <- c("software environment", "software  environment", "software\tenvironment")
texts
```

위의 `texts` 오브젝트를 공란처리를 하지 않고 `str_split()` 함수를 사용해 단어를 구분하면 어떤 문제가 발생하는가?
```{r}
library(stringr)
str_split(texts, " ")
```
두번째, 세번째는 단어가 제대로 구분되지 않았다.

이 상태에서 `sapply()` 함수를 통해 단어와 문자 수를 세어본다면?
```{r}
sapply(str_split(texts, " "), FUN = length)  # 단어 수
sapply(str_split(texts, " "), FUN = str_length)  # 문자 수
```
같은 표현임에도 불구하고 단어와 문자의 수가 다르게 나타난다.

해결방법?
1) stringr 패키지의 함수들과 regex를 이용한 해결법
2) tm package 의 `stripWhitespace()` 사용
```{r}
# 1) stringr 패키지와 regex를 활용한 공란 해결방법
text_space <- str_replace_all(texts, "[[:space:]]{1,}", " ")
text_space
sapply(str_split(text_space, " "), FUN = length)  # 단어 수
sapply(str_split(text_space, " "), FUN = str_length)  # 문자 수
```

```{r}
# tm 패키지를 이용한 공란 해결 방법
library(tm)
text_tm_space <- stripWhitespace(texts)
text_tm_space
sapply(str_split(text_tm_space, " "), FUN = length)  # 단어 수
sapply(str_split(text_tm_space, " "), FUN = str_length)  # 문자 수
```

그러나 텍스트에서 공란이 의미를 가질 수 있으므로, 공란이 가지는 의미에 대해 잘 생각하고 전처리 할 필요가 있다.

e.g. 어떤 문서에서 비공개 처리된 문장 또는 단어?


#### 2. 대소문자 통일
영어는 문장 첫 단어의 첫 문자를 대문자 처리하거나, 고유명사의 첫문자, 축약어(abbreviation)인 경우 대문자를 사용하며, 다른 경우 소문자를 사용한다. 또한 어떤 표현을 강조하고 싶을때 대문자를 사용하기도 한다.

```{r}
bush_word <- "The 43rd President of the United States of America George W. Bush hides in bush to play hide and seek with his grandchildren, yelling 'HIDE! HIDE' to them."
bush_word %>%
  str_extract_all(boundary("word")) %>%
  unlist() %>%
  table()
```

the와 The, hide와 HIDE는 내용상 차이가 없으나, 대통령 Bush와 수풀을 뜻하는 bush는 의미상 차이가 있다. 따라서 the 와 The는 대소문자를 통일하고, Bush와 bush는 대소문자를 구별하려면 어떻게 해야할까? 

```{r}
bush_word <- str_replace(bush_word, "Bush", "Bush_unique_")
# 대문자를 소문자로 변환한 뒤 빈도표 계산
table(tolower(bush_word))
```

대용량 텍스트 자료에서 의미를 가지는 대소문자를 모두 처리하기는 매우 어려운 일이지만, 빈도표를 통해 빈번하게 등장하는 단어들을 살펴보고, 자료에서 특별한 의미를 가지는 고유명사, 특수한 표현 등을 예외처리 하는 과정을 반드시 거칠 필요가 있다.


#### 3. 숫자 표현 제거
텍스트 자료에는 숫자로 표현된 자료도 포함된다. 이때 숫자가 가지는 의미는 분석자가 텍스트 자료에 대해 가지고 있는 이론적 접근 방법에 따라 달라진다. 분석자가 텍스트 내의 숫자가 의미를 가진다고 가정할 수도 있고, 숫자가 포함된 표현이라는 의미만을 부여할 수도 있으며, 숫자가 영향을 미치지 못한다고 가정할 수도 있다.

```{r}
text_number <- c("He is one of statisticians agreeing that R is the No. 1 statistical software.", "He is one of statisticians agreeng that R is the No. one statistical software.")
str_split(text_number, " ")
```

두 문장에서 숫자를 제거한다.
```{r}
text_number_erase <- text_number %>% 
  str_remove("[[:digit:]]{1,}[[:space:]]{1,}") %>%
  str_split(" ")
text_number_erase
```

숫자를 제거하고 단어 단위로 나눈 두 문장을 `str_c()`함수를 통해 문장으로 결합한다.
```{r}
str_c(text_number_erase[[1]], collapse = " ")  # 첫 번째 문장
str_c(text_number_erase[[2]], collapse = " ")  # 두 번째 문장
```
숫자를 제거하고나면 첫 번째 문장은 의미가 불명확해진 반면, 두 번째 문장은 의미가 유지된다.

숫자를 삭제하는 대신 숫자가 있었음을 표시할 수도 있다.
```{r}
text_num_replace <- text_number %>%
  str_replace_all("[[:digit:]]{1,}[[:space:]]{1,}", "_number_ ") %>%
  str_split(" ")
text_num_replace
str_c(text_num_replace[[1]], collapse = " ")  # 첫 번째 문장
str_c(text_num_replace[[2]], collapse = " ")  # 두 번째 문장
```
숫자를 완전히 삭제했을때에 비해 첫 번째 문장의 의미가 덜 모호해졌다.
그러나 두 번째 문장에 비해 완전하지는 않다.

일반적으로 텍스트 마이닝을 활용한 연구에서는 숫자를 삭제하거나, 모든 숫자를 위의 방법처럼 하나로 교체하는 사전처리를 활용한다. 자료의 성격과 자료에서 알고자 하는 결과에 대해 숙고한 뒤 숫자를 처리할 방법에 대해 결정해야 한다.


#### 4. 문장부호 및 특수문자 제거
텍스트 자료에는 많은 문장부호가 사용된다. 문장부호는 문법적, 의미론적으로 중요한 기능을 수행한다.

e.g. 마침표(.)는 일반적으로 문장의 종결을 의미함. 그런데 항상 문장의 종결을 의미하지는 않는다.

```{r}
text_punct <- "Baek et al. (2014) argued that the state of default-setting is critical for people to protect their own personal privacy on the internet."
str_split(text_punct, "\\. ")  # 마침표와 공란을 기준으로 오브젝트를 나누면?
```

"Baek et al."에서의 마침표는 마침표가 문장의 종결을 의미하지는 않는다.

단어 단위로 문장을 분리해서 문장부호가 사용된 단어들을 살펴보자.
```{r}
str_split(text_punct, " ")
```
문장부호가 포함된 단어는 "al.", "(2014)", "default-setting", "internet." 이다.
"internet."의 경우 문장의 끝을 의미하기 때문에 삭제해도 무방하다.
다음으로 "default-setting"에서 하이픈(-)은 두 단어를 이어주고 있다. 만약 하이픈을 없애면 "defaultsetting"이라는 한 단어로, 하이픈을 공백으로 대체하면 "default"와 "setting"이라는 두 단어로 변할 것이다.
다음으로 "et al."과 "(2014)"은 참고문헌을 표시하는 방법이다. 보다 정확하게 나타내면 저자의 성이 나타난 다음 공백과 "et al."이 등장하고, 다시 한 번 공백이 등장한 다음 숫자가 포함된 괄호가 나타난다. 이 패턴을 "_reference_"로 일괄 교체해보면 다음과 같다.

```{r}
# default-setting을 두 단어로 분리하고, "성 et al. (연도)"를 "_reference_로 교체"
text_punct2 <- text_punct %>% 
  str_replace_all("-", " ") %>%
  str_replace_all("[[:upper:]]{1}[[:alpha:]]{1,}[[:space:]](et al\\.)[[:space:]]\\([[:digit:]]{4}\\)", "_reference_") %>%
  str_replace_all("\\.[[:space:]]{0,}", "")
text_punct2
```


#### 5. 불용단어 제거
영어의 a, an, the와 같이 빈번하게 사용되지만, 구체적인 의미를 지니지 않은 단어를 불용단어(또는 정지단어stopword)라고 한다.

e.g. "She is a singer" vs. "She is the singer"

영어에서 불용단어를 제거하는 방법
1) 불용단어 사전을 직접 만든다.
2) tm 패키지의 `stopwords("en")`, 또는 `stopwords("SMART")`를 활용한다.
```{r}
# 1) 불용단어 사전 만들기
text_stopword <- c("She is a singer", "She is the singer")
stop_words <- "(\\ba )|(\\ban )|(\\bthe )"
str_replace_all(text_stopword, stop_words, "")
```

```{r}
# 2) tm 패키지 불용단어 사전 활용하기
library(tm)

head(stopwords("en"), 10)
length(stopwords("en"))  # en에는 174개의 불용단어 수록

head(stopwords("SMART"), 10)
length(stopwords("SMART"))  # SMART에는 571개의 불용단어 수록

# tm 패키지를 활용하는 방법은 후반부에 자세히 살펴볼 것이다.
removeWords(text_stopword, stopwords("en"))
removeWords(text_stopword, stopwords("SMART"))
```


#### 6. 어근 동일화 처리
영어와 한국어는 문법적 기능에 따라 표현이 바뀌는 경우가 많다. 예를 들어 영어에서 주어가 3인칭인 경우 동사의 형태가 변화한다. 시제에 따라 동사의 모양이 바뀌기도 한다. 한국어의 경우에도 "가다"라는 서술어는 "가고", "간", "가니", "가자" 등으로 변화한다. 어근 동일화(stemming) 처리는 파생된 형태의 단어를 동일하게 처리할 수 있도록 표현을 변환하는 텍스트 데이터 전처리 과정이다. tm 패키지를 비롯한 텍스트 마이닝 도구는 마틴 포터의 어근 동일화(Porter's Stemmer) 알고리즘을 제공한다.

tm패키지의 활용법은 뒤에에 자세히 알아보도록 하고, 먼저 어근 동일화를 가능케 하는 프로그램을 만들어보자.
```{r}
text_stemming <- c("I am a boy.", "You are a boy.", "He is a boy.", "He must be a boy.")

# 어근 동일화
text_stemming2 <- str_replace_all(text_stemming,
                                  "(\\bam )|(\\bare )|(\\bis )|(\\bwas )|(\\bwere )|(\\bbe )",
                                  "be ")
text_stemming2

table(unlist(str_split(text_stemming, " ")))
table(unlist(str_split(text_stemming2," ")))
```


#### 7. 엔그램(n-gram)
엔그램은 n번 연이어 등장하는 단어의 연쇄를 의미한다. 예를 들어 외교 정책에 대한 영문 텍스트 자료에는 '외교 정책'을 의미하는 영어 표현 'foreign policy'이 자주 등장한다. 이때 연쇄적으로 연달아 등장하는 foreign과 policy는 자주 등장할 수 밖에 없으며, 이처럼 연달아 두 단어가 등장하는 경우 특별히 바이그램(bigram) 또는 2-그램(2-gram)으로 부른다. 마찬가지로 'Republic of Korea'나 'Department of State'와 같이 세 단어가 연속적으로 등장하는 경우 트라이그램(trigram) 또는 3-그램(3-gram)으로 부른다. 이렇게 연쇄적으로 등장하는 단어들의 경우 특별한 의미를 가지고 있으므로, 한 단어로 보는 것이 타당해보인다. 

그런데 어떤 단어들의 연쇄를 한 단어로 볼 수 있을까?

e.g. Thank you, turn on, turn off, my depandent variable is ...

경우에 따라 다르다!

```{r}
text_ngram <- "The United States comprises fifty states. In the United States, each state has its own laws. However, federal law overrides state law in the United States."
text_ngram_word <- text_ngram %>%
  str_extract_all(boundary("word")) %>%
  unlist()
length(table(text_ngram_word))
sum(table(text_ngram_word))

# United States 를 2-gram으로 가정하여 처리하기.
text_ngram_word2 <- text_ngram %>%
  str_replace_all("\\bUnited States", "United_States") %>%
  str_extract_all(boundary("word")) %>%
  unlist()
length(table(text_ngram_word2))
sum(table(text_ngram_word2))

# The(the) United States 를 3-gram으로 가정하여 처리하기.
text_ngram_word3 <- text_ngram %>%
  str_replace_all("\\b(T|t)he United States", "The_United_States") %>%
  str_extract_all(boundary("word")) %>%
  unlist()
length(table(text_ngram_word3))
sum(table(text_ngram_word3))
```

n그램의 처리 방식에 따라 텍스트 자료의 복잡성이 달라지는 것을 확인할 수 있다.
1회 이상 등장하는 단어의 수가 줄어드는 것은 분석의 효율성을 증가할 수 있다.
처리해야 할 데이터의 차원 수가 감소하기 때문에, 수리모형을 적용한 결과를 훨씬 빨리 얻게 될 수 있다.

그러나 경우에 따라 n그램을 적용하는 것이 텍스트 데이터의 복잡성을 높일 수 있다.


### tm 패키지 함수를 이용한 텍스트 데이터 사전처리


#### 1. 말뭉치의 구성
```{r}
library('tm')
text_directory <- paste0(getwd(), "/ymbaek_papers")
papers <- VCorpus(DirSource(text_directory))  # 논문 24편의 초록 데이터를 불러옴
papers
```

24개의 문서가 존재함을 확인할 수 있다.

메타데이터를 확인하거나 수정하려면 `meta()`함수를 활용할 수 있다. 현재는 메타데이터가 부여되어있지 않다.

tm 패키지는 list형태로 구성되어 있는데, `summary()`함수를 통해 오브젝트 `papers`를 살펴보면 아래와 같다.

```{r}
summary(papers)
``` 

리스트로 구성되어 있기 때문에 인덱싱을 통해 각 문서를 살펴볼 수 있다. 예를 들어 `papers` 오브젝트 내에 있는 24개의 논문 초록 가운데 세 번째 문서를 살펴보는 방법은 아래와 같다.

```{r}
# 말뭉치 세 번째 문서의 내용
papers[[3]]$content
```

같은 방법으로 메타 데이터를 살펴볼 수 있다.

```{r}
# 말뭉치 세 번째 문서의 메타데이터
papers[[3]]$meta
```

#### 2. 말뭉치 전처리
tm 패키지를 활용하여 말뭉치에 대한 데이터 전처리를 할 수 있다. 말뭉치 전처리에는 아래 여섯가지 함수가 주로 활용된다.

- `removeNumbers()`: 말뭉치에 사용된 숫자 표현을 모두 제거함.
- `removePunctuation()`: 말뭉치에 사용된 문장부호 및 특수문자를 모두 제거함.
- `removeWords()`: 사전에 등록된 단어들을 말뭉치에서 제거함.
- `stripWhitespace()`: 2개 이상 연달아 등장하는 공란을 1개의 공란으로 치환함.
- `stemDocument()`: 어근 동일화 알고리즘을 적용함.
- `content_transformer()`: 이용자가 지정한 함수를 적용함.
  e.g. `content_transformer(tolower)`는 말뭉치의 모든 대문자를 소문자로 치환함.
  
위의 여섯가지 함수들을 tm 패키지의 `tm_map()` 함수에 적용하면 말뭉치를 구성하고 있는 텍스트를 효율적으로 처리할 수 있다. 

그러나 앞서 강조한 바와 같이 전처리 과정을 통해 텍스트의 성격이 변하거나 중요한 의미가 상실되지 않도록 먼저 텍스트를 면밀히 살펴볼 필요가 있다.

e.g. 특수문자가 포함된 단어들을 살펴보는 방법
```{r}
# 문자 사이에 특수문자가 포함된 단어를 추출하는 함수 
punct_check <- function(x) {
  str_extract_all(x, "[[:alnum:]]{1,}[[:punct:]]{1}?[[:alnum:]]{1,}")
}
punct_check_papers <- lapply(papers, FUN = punct_check)
table(unlist(punct_check_papers))
```


e.g. 숫자가 사용된 표현을 살펴보는 방법
```{r}
num_check <- function(x) {
  str_extract_all(x, "[[:digit:]]{1,}")
}
num_check_papers <- lapply(papers, FUN = num_check)
table(unlist(num_check_papers))
```


e.g. 대문자로 시작하는 단어를 확인하는 방법 (고유명사 확인에 유용함)
```{r}
upper_check <- function(x) {
  str_extract_all(x, "[[:upper:]]{1}[[:alpha:]]{1,}")
}
upper_check_papers <- lapply(papers, FUN = upper_check)
table(unlist(upper_check_papers))
```


위의 점검 과정을 통해 아래 내용을 교체하기로 하였다.

- 숫자표현 모두 삭제
- 특수문자는 두 단계를 거쳐 삭제
    + 아래 표에 따라 몇몇 표현을 교체
    + 표에 나타나지 않은 특수문자는 일괄 삭제

|지정된 표현|교체된 표현|
|-----------|-----------|
|-collar|collar|
|co- 또는 Co-|co|
|cross- 또는 Cross-|cross|
|meta- 또는 Meta-|meta|
|opt- 또는 Opt-|opt|
|post- 또는 Post-|post|
|inter- 또는 Inter-|inter|
|within- 또는 Within-|within|
|-end|end|
|ICD-|ICD|
|K-pop|Kpop|
|e.g.|for example|
|i.e.|that is|
|'s|삭제|
|`s|삭제|
|and/or|and or|
|his/her|his her|
|=|is equal to|
|나머지 -의 경우|스페이스 공란(' ')|


- 2번 이상 연이어 나타난 공란은 하나의 스페이스 공란(" ")으로 교체
- 대문자는 모두 소문자로 전환
- tm 패키지의 SMART 불용문자 사전에 있는 단어를 모두 삭제
- 어근 동일화 알고리즘 적용
- 엔그램 처리하지 않음

1) 숫자표현 삭제
```{r}
paper_corpus <- tm_map(papers, removeNumbers)
```

2) 특수문자 지정표현 교체
```{r}
# removePunctuation 함수는 텍스트 자료의 맥락을 고려하지 않고 특수문자를 삭제
# 따라서 맥락을 고려한 특수문자 변화를 위해 아래 방법을 사용할 수 있다.
punct_change <- function(x, oldexp, newexp) {
  new_x <- tm_map(x,
                  content_transformer(function(x, pattern) gsub(pattern, newexp, x)), oldexp)
  new_x
}
paper_corpus <- paper_corpus %>%
  punct_change("-collar", "collar") %>%
  punct_change("\\b(c|C)o-", "co") %>%
  punct_change("\\b(c|C)ross-", "cross") %>%
  punct_change("\\b(m|M)eta-", "meta") %>%
  punct_change("\\b(o|O)pt-", "opt") %>%
  punct_change("\\b(p|P)ost-", "post") %>%
  punct_change("\\b(i|I)nter-", "inter") %>%
  punct_change("\\b(w|W)ithin-", "within") %>%
  punct_change("-end", "end") %>%
  punct_change("ICD-", "ICD") %>%
  punct_change("K-pop", "Kpop") %>%
  punct_change("e\\.g\\.", "for example") %>%
  punct_change("i\\.e\\.", "that is") %>%
  punct_change("\\'s", "") %>%
  punct_change("`s", "") %>%
  punct_change("and/or", "and or") %>%
  punct_change("his/her", "his her") %>%
  punct_change("=", "is equal to") %>%
  punct_change("-", " ")

# 특수문자를 의도에 따라 변경한 뒤 나머지 특수문자 확인
temp_check <- lapply(paper_corpus, FUN = punct_check)
table(unlist(temp_check))
```

다음으로 `tm_map(x, removePunctuation)`을 활용하여 특수문자를 제거하였다.
```{r}
paper_corpus <- tm_map(paper_corpus, removePunctuation)
```


3) 2개 이상 연이어 사용된 공란을 하나의 공란으로 처리
```{r}
paper_corpus <- tm_map(paper_corpus, stripWhitespace)
```


4) 대소문자 통합
```{r}
paper_corpus <- tm_map(paper_corpus, content_transformer(tolower))
```


5) 불용단어 삭제 (SMART 사전 사용)
```{r}
paper_corpus <- tm_map(paper_corpus, removeWords, words = stopwords("SMART"))
```
만약 연구자가 직접 생성한 불용단어 사전이 있다면 `words = stopwords("")`에서 ""에 해당 사전 오브젝트를 입력하면 된다.

6) 어근 동일화
```{r}
paper_corpus <- tm_map(paper_corpus, stemDocument, language = "en")
```


전처리 전(`papers`)과 전처리 후(`paper_corpus`)의 말뭉치를 비교해보기
```{r}
char_fun <- function(x) {
  str_extract_all(x, ".")
}
word_fun <- function(x) {
  str_extract_all(x, boundary("word"))
}
count_fun <- function(x) {
  char_obj <- lapply(x, FUN = char_fun)
  unique_char <- length(table(unlist(char_obj)))
  total_char <- sum(table(unlist(char_obj)))
  word_obj <- lapply(x, FUN = word_fun)
  unique_word <- length(table(unlist(word_obj)))
  total_word <- sum(table(unlist(word_obj)))
  result <- rbind(unique_char, total_char, unique_word, total_word)
  print(result)
}

# 전처리 전후 비교
before_paper <- count_fun(papers)
after_paper <- count_fun(paper_corpus)
result_comparing <- data.frame(before_paper, after_paper)
result_comparing
```


#### 문서X단어 행렬, 단어X문서 행렬
텍스트 분석 연구를 진행할 때는 말뭉치에 대해 사전처리 과정을 거친 뒤, '문서X단어 행렬(DTM, Document-Term Matrix) 또는 '단어X문서 행렬(TDM, Term-Document Matrix)'을 만들어 추가적인 통계 분석을 실시하게 된다. DTM과 TDM 모두 특정 문서에 등장하는 특정 단어의 등장 빈도를 행렬로 나타낸 것이다. DTM은 가로줄에 문서가, 세로줄에 단어가 배치된 행렬이며, TDM은 가로줄에 단어가, 세로줄에 문서가 배치된 행렬이다. (즉, DTM과 TDM은 서로 전치행렬임)

tm 라이브러리에서 말뭉치를 DTM으로 나타내려면 `DocumentTermMatrix()` 함수를, TDM으로 나타내려면 `TermDocumentMatrix()`를 사용하면 된다.


```{r}
# 문서X단어 행렬 구축하기
dtm_papers <- DocumentTermMatrix(paper_corpus)
dtm_papers
```

`dtm_papers` 오브젝트는
- 총 24개의 문서와 686개의 단어들로 구성되어있다.
- Non-/sparse entries : 1379/15085는 빈도 정보가 제공된 칸과 그렇지 않은 칸의 수이다. 즉 1379개의 칸은 최소 1회 이상의 빈도가 발견되었지만, 15085개의 칸에는 0회의 빈도수가 나타났다는 뜻이다.
- sparsity : 92%는 전체 칸에서 92%가 0의 빈도수를 가진다는 뜻이다.
- Maximal term length: 17 은 가장 긴 문자 수를 가지는 단어의 문자수가 17이라는 의미이다.
- Weighting : term frequency(tf)는 DTM의 칸에 투입된 수치가 단어 빈도임을 나타낸다.


인덱싱을 통해 DTM의 가로줄, 세로줄을 추출할 수 있다.
```{r}
rownames(dtm_papers[,])  # 행 이름: 문서
colnames(dtm_papers[,])  # 열 이름: 단어
```


`inspect()` 함수를 사용해 행렬의 일부분을 살펴볼 수 있다.
```{r}
# 행렬의 일부분 살펴보기
inspect(dtm_papers[1:4, 60:65])
```

